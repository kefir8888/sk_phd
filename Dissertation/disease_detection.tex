\chapter{Disease detection}

%\section{Thesis Structure}

% =======================================================
%# Modern greenhouses #
% =======================================================
Modern agriculture poses several challenges in terms of the requirements for monitoring speed and precision.
%old sentence
%After a certain size of the greenhouse, that is already reached by several major agricultural companies, it becomes virtually impossible to timely examine the whole greenhouse facility with a reasonable human workforce.
Certain greenhouses of the major agricultural companies are so huge, that it is virtually impossible to timely examine the whole facility with a reasonable human workforce.
While it is possible to introduce more and more human workers, a sustainable long-term solution is to use autonomous robots to automate such repetitive tasks.

While robot-assisted agriculture has been developing for a long time already, the integration of a robot into an industrial-scale production often demands substantial effort.
It requires the robot to have a high degree of autonomy while keeping the modifications that the greenhouse facility should go through to the minimum.
% =======================================================
\subsection{Target environment}
% =======================================================
The target greenhouses for this project are rectangular 120000 \si{m^2} (12 hectares) buildings with glass rooftops.
During the night and in cloudy weather red-blue LEDs are used to provide sufficient lighting.
The temperature and humidity are kept stable at all times. Several types of tomatoes are grown there with the help of hydroponics. One tomato plant grows up to 35 meters in length during 10 months of cultivation.
The majority of the length of the plant is kept horizontal, and the top of the plant is at a height of approximately 4 meters.
% =======================================================
\subsection{Powdery mildew}
% =======================================================
%old sentence
%The density of the biomass (see Figure \ref{fig_greenhouse_top_view}) in these greenhouses allows for cost savings because of the scale of the production, but, on the other hand, it leads to the rapid spread of various kinds of bacterial infections, fungi, viruses, and insects.
The density of the biomass (see Figure \ref{fig_greenhouse_top_view}) in these greenhouses allows for cost savings because of the scale of the production. 
On the other hand, it leads to the rapid spread of various kinds of bacterial infections, fungi, viruses, and insects. Thus, timely monitoring in such greenhouses is crucial.
Powdery mildew is known to infect the lower leaves first and then spread up the plant.
It develops best at temperatures slightly below 30\si{\celsius} and high humidity, which are precisely the conditions that the tomatoes are grown in for the great portion of the production cycle.
Thus, the development of the powdery mildew is rapid, taking 4 to 7 days to progress from early to medium stage \cite{WSPANIALY2016487}.


With the aforementioned requirements for monitoring speed in mind, an autonomous robot on an omnidirectional platform was developed and manufactured.
It is equipped with cameras, covering the whole height of the tomato plant, and an onboard computer to process the data in real-time.

This paper presents a work on the data acquisition with this robot, as well as coordinating the markup procedure by human experts and a comparison of the performance of several neural networks.
% =======================================================
%# State of the field
% =======================================================
\subsection{Datasets available and related work}

The majority of the open datasets are either small or taken in laboratory conditions, meaning professional cameras, artificial lighting, separation of the leaves from the plant, or manual preprocessing.
By far the most popular dataset in the field \cite{hughes2015open} consists of images of leaves on a contrastive background.
Collecting such a dataset is expensive, and its usage is limited by the differences between the laboratory and real conditions.

While there are several publicly available datasets out there, relying on them could be risky due to the covariance shift \cite{sugiyama2008direct}.
The main difficulties for obtaining qualitative and consistent markup are\cite{barbedo2016review}:
\begin{itemize}
    \item Symptom variations.
    \item The potential presence of different disorders with similar symptoms.
    \item Ill-defined edge cases, leading to the markup ambiguity.
\end{itemize}Taking into account all the aforementioned factors, a dataset for classification, consisting of images from the target greenhouse in normal conditions was collected and marked, see section \ref{Markup}.
It is worth noting that accuracy is by far the most used evaluation metric in the field of automatic plant disease detection \cite{teeffelen2023detection}.

% =======================================================
%# Contribution  #
% =======================================================

\subsection{Related work}

There are multiple solutions to the same problem that were stated in different environments.
They can rely on hyperspectral imaging \cite{zhang2023detection}, \cite{fernandez2021cucumber}, \cite{PowderyZiheng}, \cite{abdulridha2020detecting}, on recurrent NNs \cite{varshney2021deep}, a custom convolutional architecture \cite{lin2019deep}.
Hyperspectral cameras are usually expensive, which limits their availability.
In this work, relatively cheap user-grade RGB cameras were used.

There are several works that are aimed at comparing the performance of already widely adopted models on specific tasks, such as \cite{PowderyBOYAR}, \cite{9566132}, and \cite{augmentationShin}.
The aim of this work is to produce a solution that fits the specific needs of the environment from an engineering standpoint, see section \ref{Overview of the solution}.

\subsection{Contribution}

The contributions in the field of disease detection are as follows:
\begin{itemize}
    \item Roboticised data acquisition was performed. Namely, a robot was designed and manufactured, and a dataset of powdery mildew-infected tomato leaves was gathered. The data collection was performed in a real environment under varying lighting conditions.
    \item Marking of the 6817 images into positive and negative classes was coordinated. The markup was performed by human experts with formal education in agriculture. The consistency of the experts was assessed by providing the same samples to different experts.
    \item Using this data, an experiment was conducted: YOLOv8 (n, s, m), EfficientNet (V2l, B3, B4), ResNet (34 and 152), MobileNetV3 (small and large) were compared in terms of their accuracy and the inference time on the target computer.
    \item It was thus demonstrated that good enough performance could be achieved with the means of user-grade cameras and images that were taken from the moving robot. The mutual expert consistencies were matched by the NN model in terms of the classification accuracy, see section \ref{sec_results_disease}.
\end{itemize}

The chapter is structured as follows.
First, the mechanical platform is briefly described and an overview of the solution to the disease detection problem is given.
Second, the data acquisition and markup procedures are described.
Third, the training in terms of the models, parameters, and experiments is described.
Finally, a comparison of the performance of different NN models is given.

\section{Training}
\label{sec_training}
\subsection{Hardware}
A two-tier computing strategy for training and testing the neural networks was used.
The training was performed on a stationary server: Intel Core i9-9900 3.10 \si{GHz}, GeForce RTX 3080, 32 \si{Gb}.
Testing was carried out on the target device - the robot's onboard computer. Its performance is much more limited: Intel Core i5-11400H 2.70 \si{GHz}, GeForce RTX 3050Ti laptop, 16 Gb.
Both machines operated under Ubuntu 20.04.

\subsection{Models}
A comparison of 10 top-performing classification models provided by Roboflow \cite{lin2022roboflow} was carried out. These models encompass:

\begin{enumerate}
    \item Three versions of YOLOv8\cite{jocher2022ultralytics}: YOLOv8n, YOLOv8s, and YOLOv8m.
    \item Three versions of EfficientNet\cite{tan2019efficientnet}: EfficientNetV2l, EfficientNetB3 and EfficientNetB4.
    \item Two versions of ResNet\cite{he2016deep}: ResNet34 and ResNet152.
    \item Two versions of MobileNet\cite{howard2017mobilenets}: MobileNetV3small and MobileNetV3large.
\end{enumerate}

\subsection{Data sampling}
\label{Data_sampling}
Three distinct datasets were created, each addressing the class imbalance challenge within the original dataset. Initially, the data comprised 5684 instances of the negative class and 1133 instances of the positive class.
To tackle the imbalance, three different strategies were pursued:
\begin{itemize}
    \item For the first set 1133 images of the negative class were randomly selected, equalizing the number of photos in both classes.
    \item For the second set all the photos from the positive class were duplicated to increase the number of original images of the negative class. The number of training photos per class grew from 850 to 1700.
    \item For the third set, all the labeled data available was used. To achieve this, extensive and diverse augmentation techniques were applied. The positive class images were duplicated six times with augmentation, and similar augmentation transforms were applied to $\frac{6}{7}$ of the negative class images. This training set consists of 5361 instances of the positive class and 5316 instances of the negative class.
\end{itemize}

In all the experiments 75-15-10 \% train-val-test split was used.
\subsection{Data Preprocessing}
\textit{Resizing}: All the images underwent cropping to a square shape and a resizing process with the resultant dimensions of 1056 $\times$ 1056 pixels.
The rationale behind adopting this specific size is the following.
First, some of NN models that were planned to be used could work only with the image sizes that are multiples of 32.
Second, the downscaling of the images was not feasible, because powdery mildew appears with quite small features, and reducing the image size would entail a substantial loss of critical information.
Third, resizing rectangular images into a square will again lead to the loss of data.

\textit{Augmentation}: Across all three datasets, a standardized suite of transformations was applied.
These transformations encompassed horizontal and vertical flipping, shifting, rotation, and HSV color transformation.
However, it is worth noting that the third dataset underwent a more expansive augmentation strategy.
For this subset, the following augmentations were applied:
\begin{itemize}
    \item \textit{Predominantly:} Defocus, stochastic mist, noise injection, RandomBrightnessContrast, and HSV color transformation featuring augmented values.
    \item \textit{To a smaller extent:} ColorJitter, rgbshift, ChannelShuffle, ChannelDropout, image inversion, and sepia.
\end{itemize}
The selection of these augmentation techniques aimed to diversify the dataset as much as possible to enhance the model's generalization capabilities.
\subsection {Experimental setup}

The setup in terms of the training is the following:
\begin{itemize}
    \item Training the majority of the models for 40 epochs before choosing the best checkpoint. The exact number for each model is given in the table \ref{tabularx:models}.
    \item SGD optimizer was used.
    \item The default values for the learning rate of 0.01 for YOLO models and 0.001 for other models were used.
\end{itemize}

The choice on the number of epochs was made following preliminary training runs with the learning curves stagnating after such a number of epochs.
The batch size for all models was determined out of the memory constraints.

\section{Results}
\label{sec_results_disease}

\begin{landscape}
\begin{table}[ht]
    \centering    
    \begin{tabularx}{0.99\linewidth}{ 
    | >{\centering\arraybackslash}X 
    | >{\centering\arraybackslash}X 
    | >{\centering\arraybackslash}X 
    | >{\centering\arraybackslash}X 
    | >{\centering\arraybackslash}X 
    | >{\centering\arraybackslash}X 
    | >{\centering\arraybackslash}X | }
    \hline
    Model & Params & Accuracy [\%] & Server inference [ms] & Onboard PC inference [ms] &
    Training time & Epochs \\ \hline
    Resnet34 & 21.8M & 84.40 & 34.7 & 38.23 & 169m 40s & 40 \\ \hline
    Resnet152 & 60.2M & 85.40 & 34.00 & - & 96m 58s & 40 \\ \hline
    YOLOv8n & 3.2M & \textbf{85.80} & 5.9 & \textbf{6.6} & 25m 54s & 30 \\ \hline
    YOLOv8s & 11.2M & 85.00 & 10.6  & 12.5 & 27m 37s & 30 \\ \hline
    YOLOv8m & 25.9M & 84.50 & 30.7 & 33.0 & 29m 40s & 30 \\ \hline
    EfficientNetV2l & 118.5M & 84.07 & 49.3 & - & 124m 19s & 40 \\ \hline
    EfficientNetB3 & 12.2M & 80.53 & 14.86 & - & 66m 45s & 40 \\ \hline
    EfficientNetB4 & 19.3M & 79.20 & 19.7 & - & 74m 29s & 40 \\ \hline
    MobilenetV3s & 2.5M & 83.63 & 4.15 & 6.64 &  54m 23s & 40 \\ \hline
    MobilenetV3 & 5.5M & 84.51 & 4.81 & 19.21 & 52m 23s & 40 \\ \hline
    \end{tabularx}
    \caption{Performance of all models on the first dataset.}
    \label{tabularx:models}
\end{table}

\vspace{1em}

\begin{table}[ht]
    \centering
    \begin{tabularx}{\linewidth}{ 
        | >{\centering\arraybackslash}X 
        | >{\centering\arraybackslash}X 
        | >{\centering\arraybackslash}X 
        | >{\centering\arraybackslash}X 
        | >{\centering\arraybackslash}X 
        | >{\centering\arraybackslash}X 
        | >{\centering\arraybackslash}X | }
        \hline
    Model & Accuracy & Inference server (ms) & 
    Inference (ms) & Training time & Epochs & Batch \\ \hline
    YOLOv8n & 85.4 & 5.9 & 6.6 & 83m 52s & 40 & 16 \\ \hline
    MobilenetV3s & 84.07 & 4.2 & 6.61 & 182m 59s & 40 & 32 \\ \hline
    MobilenetV3l & 82.74 & 4.87 & 19.41 & 357m 19s  & 40 & 32 \\
    \hline
    \end{tabularx}
    \caption{Performance of selected models on the second dataset.}
    \label{tabularx:models_extended_dataset}
\end{table}

\vspace{1em}

\begin{table}[ht]
    \centering
    \begin{tabularx}{\linewidth}{ 
        | >{\centering\arraybackslash}X 
        | >{\centering\arraybackslash}X 
        | >{\centering\arraybackslash}X 
        | >{\centering\arraybackslash}X 
        | >{\centering\arraybackslash}X 
        | >{\centering\arraybackslash}X 
        | >{\centering\arraybackslash}X | }
        \hline
    Model & Accuracy & Inference server (ms) &
    Inference (ms) & Training time & Epochs & Batch  \\ \hline
    YOLOv8n & 77.5 & 3.3 & 7.3 & 472m 32s & 40 & 16 \\ \hline
    MobilenetV3s & 77.45 & 4.08 & 6.65 &  293m 1s & 40 & 16 \\ \hline
    MobilenetV3l & 77.45 & 4.95 & 19.16 & 321m 32s & 40 & 16 \\
    \hline
    \end{tabularx}
    \caption{Performance of selected models on the third dataset.}
    \label{tabularx:models_aug_dataset}
\end{table}
\end{landscape}

Overall, a dataset of powdery mildew-infected leaves was collected with the user-grade RGB cameras.
A number of neural networks were trained with the best of them performing on par with the human experts.
The inference time on the onboard computer is well below the maximal acceptable time, thus leaving room for the introduction of other models or increasing the number of cameras.

%Further experiments were carried out with them.
\begin{figure}[ht] 
    \centering
    \includegraphics[width=0.6\textwidth]{images/server_models.pdf}
    \caption{Performance and FPS of the models trained on the first dataset with inference on the server.}
    \label{fig:Firstresult}
\end{figure}
\begin{figure}[ht] 
    \centering
    \includegraphics[width=0.6\textwidth]{images/laptop_models.pdf}
    \caption{Performance and FPS of the chosen best models trained on the first dataset with inference on the onboard laptop.}
    \label{fig:Secondresult}
\end{figure}


Table \ref{tabularx:models} presents the results of the training of all the models considered on the first version of the dataset, see Section \ref{Data_sampling}.
It was not possible to run certain models on the onboard laptop, taking into account the target resolution of 1056 $\times$ 1056. These models are marked with a dash sign in the \textit{Onboard PC inference} column.
These models were excluded from further experiments because they were too demanding in terms of computational resources.
%The remaining models were examined from the standpoint of the combination of accuracy and inference time, see Figure \ref{fig:Firstresult} for results on the server and Figure \ref{fig:Secondresult} for results on the target machine.

All the models were examined from the standpoint of the combination of accuracy and inference time, see Figure \ref{fig:Firstresult} for the results on the server.
It turns out, that three of them are Pareto-optimal, meaning that no other model is faster and more precise at the same time.
Those are YOLOv8n, MobilenetV3small and MobilenetV3large.
Figure \ref{fig:Secondresult} gives their results on the target machine.

%The second version of the dataset includes more original photos of the negative class and duplicates of photos of the positive class.
%Due to the increase in the size of the dataset, the training time has also increased.
%At the same time, the accuracy increased only for the Mobile net V3 small model. \ref{fig:Second result}

Another series of experiments was carried out with three aforementioned models and the second version of the dataset, see Table \ref{tabularx:models_extended_dataset}.
The results did not subvert the expectations with accuracy slightly increasing for MobilenetV3small and slightly dropping for other models.

With the third version of the dataset the accuracy values have become significantly worse, see Table \ref{tabularx:models_aug_dataset}.
The values of the metrics for different models are relatively close to each other, matching the consistency of the experts in terms of accuracy.
Thus, the reasonable qualitative explanation could be that the performance is limited more or less by the data quality, not the models' generalization capabilities.
The best performance by both accuracy and time was shown by YOLOv8n neural network.
The False Negative rate for it is 6 \%.

In order to assess the model performance on the high-quality data, a set of 100 images was labeled by a consensus of 4 experts.
The consensus here means that the image-wise labels were agreed upon by all the experts.
The output of the model matched the expert labeling for 96 samples of 100.

The overall performance in terms of speed is well above the minimal requirements, meaning that the monitoring speed is limited by the robot's motion (because of safety concerns), not by the inference time of the neural networks.

It is worth noting that this excessive performance could be beneficial for solving other related problems. For instance, to detect certain tiny insects it will be necessary to install much more cameras with small FoV. Their number could rise to dozens, requiring very low inference time for a single image. These cases could be addressed without any substantial modification to the vision pipeline.

% ==================
%# IV. CONCLUSION #
% ==================
%\section{Conclusion}

%\subsection{Conclusion}

